{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d90a2e",
   "metadata": {},
   "source": [
    "# Project 1: Data analysis\n",
    "\n",
    "## 1a) Bayesian approch\n",
    "\n",
    "In order to predict the effect of genes on symptoms, we will use a simple Bayesian model. We consider these hypothesis:\n",
    "\n",
    "$$\\mu_0: P(symptoms|genes) = P(symptoms)$$\n",
    "$$\\mu_1: P(symptoms|genes) \\neq P(symptoms)$$\n",
    "\n",
    "In order words, does the genes have any effect on the symptoms or not? If it does, which genomes are the most relevant for the preventing the symptoms? \n",
    "\n",
    "\n",
    "We let Y be the symptoms and our X the genes. Y is an 8 bit vector for each observation,with 1 or zero for each bit. X is an 128 with the same charectaristic as Y.\n",
    "\n",
    "\n",
    "The probability distribution of $ Y $ depends on an unknown parameter that we assume to be stochastic. We call this unknown parameter $\\theta$ or $ p $. \n",
    "\n",
    "The formulation under is an alternative formulation of the the previous hypotesis. Does the parameter that the probability distribution of Y depend on, depend on X or not. $ \\mu_0 $, our null, says that it does not. $\\mu_1 $, the alternative hypothesis claims it does. We assume the Beta-Bernoulli model:\n",
    " \n",
    "$$\\mu_0: \\theta^0 \\sim Beta(\\alpha^0 , \\beta^0) ,  \\ \\ \\  Y | \\theta^0  $$\n",
    "$$\\mu_1: \\theta^{1,x_t} \\sim Beta(\\alpha^{1,x_t} , \\beta^{1,x_t}) , \\ \\ \\ y_t | x_t \\sim bernoulli(\\theta^{1,x_t}) $$\n",
    " \n",
    "\n",
    "Given the data, D, we try to estimate\n",
    "\n",
    "$$ P(\\mu | D) = \\frac{P_{\\mu} (D) \\cdot \\xi(\\mu)}{\\sum_{i=1} ^n P_{\\mu_1}(D) \\cdot \\xi(\\mu_1)} $$\n",
    "\n",
    "We can either set $P(\\mu | D)$ to mean $P(\\mu_1 | D)$ or  $P(\\mu_0 | D)$ depending on which hypothesis we want to check the posterior belief of. \n",
    "\n",
    "We let $P_{\\mu} (D) = P(\\mu |D)$, $D = (x_t, y_t,x_{t-1},y_{t-1})_{t=1} ^T $ for a spesific gene, and let $x_t$ be the value of the gene for observation t (0  or 1), $x_{t-1}$ is a vector of all the previous values of the previous observation up to t-1,  $y_t$ is the the value of symptom Y for observation t (0 or 1),$y_{t-1}$ is a vector of all the previous values of the previous observations including t-1 and $\\mu$ is one of the hypothesis in the set {$\\mu_1$,$\\mu_0$}.  We assume that the different observations(rows) of the data are independent of each other. This gives us:\n",
    "$$ P_{\\mu}(D) = P(y_1,...,y_t \\cup x_1,...,x_t)$$\n",
    "\n",
    "$$ = \\prod_{i=1}^{t} P(y_i | x_i,x_{i-1} , y_{i-1}) $$\n",
    "\n",
    "\n",
    "With these assumptions, we can estimate the terms in the $P_{\\mu} (D)$ by \n",
    "$$ P(y_t | x_t,x_{t-1} , y_{t-1}) = \\int_{\\theta} P_{\\theta}(y_t|x_t) \\cdot \\xi(\\theta|x_{t-1},y_{t-1})$$\n",
    "\n",
    "We calculate the likehood of adding a new observation(row) using this formula. It works if we assume that the the next observation is independent from the previous observations.\n",
    "\n",
    "\n",
    "This is the marginal likelihood(posterior predictive distribution) which is a compound distribution,which is used to calculate $P(y_t | x_{t},x_{t-1}, y_{t-1})$. The probabilty $P(y_t | x_{t})$ is stochastic in $\\theta$, so we have to integrate over all values of $\\theta$. Every time a new observation is added we calculate $P(y_t | x_{t},x_{t-1}, y_{t-1})$,and recalculate $$ = \\prod_{i=1}^{t} P(y_i | x_i ,x_{i-1}, y_{i-1}) $$. \n",
    "\n",
    "So to calculate $$ P(\\mu | D) = \\frac{P_{\\mu} (D) \\cdot \\xi(\\mu)}{\\sum_{i=1} ^n P_{\\mu_i}(D) \\cdot \\xi(\\mu_i)} $$, we are sequentially updating our belief every time a new observation is added. For every update $\\xi(\\mu)$, $P(\\mu | D)$ is from the previous iteration, exept for the first one. $\\xi(\\mu)$ will cancel because it's constant.\n",
    "\n",
    "This integral simplifies to:\n",
    "\n",
    "$$ P(y_t | x_{t},x_{t-1},y^{t-1}) = \\int_{\\theta} P_{\\theta}(y_t) \\cdot d \\xi(\\theta|y^{t-1}) = \\frac{\\alpha_{t,x_t}}{\\alpha_{t,x_t} + \\beta_{t,x_t}}$$\n",
    "\n",
    "\n",
    "Looking at $\\mu_1$ we are using the Beta-Bernoulli distribution. Its a binomial distribution where the probability of success at each of n trials is not fixed but comes from a Beta distribution. The Bernoulli distribtuion is a special case of the binomial distribution, where number of trials is equal to 1.\n",
    "\n",
    "We are doing this because our y's are either 1 or 0. It is then reasonable to assume that $Y|X$ is Bernoulli distributed.\n",
    "\n",
    "It is also conviniant because of its conjugate prior property. Now we can calculate $P( D | \\mu_1) = \\prod_{i=1}^{t} P(y_i | x^i ,x^{i-1}, y^{i-1}) $ beacuse we know that the marginal likelihood (posterior predictive dist) is composed of a Bernoulli distribution and a Beta dist $ P(y_t | x^t , y^{t-1}) = \\int_{\\theta} P_{\\theta}(y_t|x_t) \\cdot \\xi(\\theta|x^{t-1},y^{t-1})$, which is beta distributed. For each iteration we end up getting that $P_{\\mu} (D)$ is Beta distributed.\n",
    "\n",
    "The same integral calculation is done for $P(D | \\mu_0)$ but now the marginal likelihood is independent of the data.\n",
    "\n",
    "Instead of calculating $P(D |\\mu_1) = \\prod_{i=1}^{t} P(y_i | x_i ,x_{i-1}, y_{i-1})$, we can first calculate $log(P(  D|\\mu_1))$ and then do $e^{log(P(D|\\mu_1))}$,same for $P(D | \\mu_0)$.\n",
    "\n",
    "To summarize, we have a set of hypotesis. In this case two. We want to find how accurate they are, so we have a prior beleif on the strength of the hypothses which we express through a probability. We then want to update our belief of the hypotesis (posterior belief). In other words what is the probability of the choosen hypotesis given the data. We only have to compute one of the probabilities because $P(\\mu_1)= 1 - P(\\mu_0)$.\n",
    "\n",
    "The calculation is done sequentially. For each iteration we get a new data observation.A new row of genes.We continously update the posterior belief,for all genes,and put the posterior belief of each gene into a list. The belief get updated through calculating the marginal likelihood every time a new row is added and a product is taken between this marginal likelihood and all the presious marginal likelihood calculations bought for $P(  D|\\mu_1)$ and for$P(  D|\\mu_0)$ this is then put into the bayes formula $ P(\\mu | D) = \\frac{P_{\\mu} (D) \\cdot \\xi(\\mu)}{\\sum_{i=1} ^n P_{\\mu_i}(D) \\cdot \\xi(\\mu_i)} $.\n",
    "\n",
    "We want to find this probability for for each of the genes.This procedure is done for each gene.For each gene if the posterior probabilty is greater then some trechhold we add this gene to the list of important genes that are relvant for explaining that spesific symptom.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "The decision rule can be defined as\n",
    "\n",
    "$$ \\mu =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\mu_1 & P(\\mu_1|D) > s \\\\\n",
    "      \\mu_0 & else\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "where $s$ is a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3898c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22a29b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "24bec60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter('ignore', (UserWarning, np.VisibleDeprecationWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8be2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (['Covid-Recovered', 'Covid-Positive', 'No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death', \n",
    "        'Age', 'Gender', 'Income'] + \n",
    "         [f'Gene_{i+1:03}' for i in range(128)] + \n",
    "         ['Asthma', 'Obesity', 'Smoking', 'Diabetes', 'Heart disease', 'Hypertension',\n",
    "         'Vacc_1', 'Vacc_2', 'Vacc_3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0c4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = pd.read_csv(\"observation_features.csv\")\n",
    "treat_data = pd.read_csv(\"treatment_features.csv\") \n",
    "action_data = pd.read_csv(\"treatment_actions.csv\")\n",
    "outcome_data = pd.read_csv(\"treatment_outcomes.csv\") \n",
    "\n",
    "obs_data.columns = cols\n",
    "treat_data.columns = cols\n",
    "outcome_data.columns = cols[:10]\n",
    "action_data.columns = ['Treatment_1', 'Treatment_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2c9f11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self, X, y, random_state=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def generate_binary_data(self, num_features, N, correlation=[0.9, 0.5]):\n",
    "        \"\"\"\n",
    "        From a number of features and one correlation vector,\n",
    "        create a data set with N observations and num_features,\n",
    "        with one target column that correlates with the features,\n",
    "        as given in the correlation vector\"\"\"\n",
    "        \n",
    "        data = np.random.choice(2, size=(N, num_features))\n",
    "        df = pd.DataFrame(data)\n",
    "        df[\"Target\"] = np.zeros(N).astype(int)\n",
    "        for i, cor in enumerate(correlation):\n",
    "            if i >= num_features:\n",
    "                break\n",
    "                \n",
    "            df[\"Target\"] |= df.iloc[:, i] * np.random.choice(2, size=N, p=[(1-cor), cor])\n",
    "            \n",
    "        return df.iloc[:, :num_features], df[\"Target\"]\n",
    "    \n",
    "    \n",
    "    def select_features(self, X, Y, threshold):\n",
    "        \"\"\" Select the most important features of a data set, where X (2D)\n",
    "        contains the feature data, and Y (1D) contains the target\n",
    "        \"\"\"\n",
    "        X, Y = np.array(X), np.array(Y)\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        n_data =  X.shape[0]\n",
    "        alpha_b = np.ones([n_features, 2 ])\n",
    "        beta_b = np.ones([n_features, 2])\n",
    "        log_p = np.zeros(n_features)\n",
    "\n",
    "        log_null = 0\n",
    "        alpha = 1\n",
    "        beta = 1  \n",
    "        for t in range(n_data):\n",
    "            p_null = alpha / (alpha + beta)\n",
    "            log_null += np.log(p_null)*Y[t] + np.log(1-p_null)*(1 - Y[t])\n",
    "            alpha += Y[t]\n",
    "            beta += (1 - Y[t])\n",
    "            for i in range(n_features):\n",
    "\n",
    "                    x_ti = int(X[t,i])\n",
    "                    p = alpha_b[i, x_ti] / (alpha_b[i, x_ti] + beta_b[i, x_ti])\n",
    "                    log_p[i] += np.log(p)*Y[t] + np.log(1-p)*(1 - Y[t])\n",
    "                    alpha_b[i, x_ti] += Y[t]\n",
    "                    beta_b[i, x_ti] += (1 - Y[t])\n",
    "        log_max=np.mean(log_p)\n",
    "        log_max2=np.mean(log_null)\n",
    "        log_p=log_p-log_max\n",
    "        log_null=log_null-log_max2\n",
    "        #p = np.exp(log_p) / (np.exp(log_p) + np.exp(log_null))\n",
    "        p = 1 / (np.exp(log_null - log_p) + 1)\n",
    "        #print(f\"{(log_p)=}\\n{(log_null)=}\\n{(log_p) + (log_null)=}\\n {p=}\")\n",
    "        #print(f\"{np.exp(log_p)=}\\n{np.exp(log_null)=}\\n{np.exp(log_p) + np.exp(log_null)=}\")\n",
    "\n",
    "        features = [i for i in range(n_features) if p[i] > threshold]\n",
    "\n",
    "        return features\n",
    "    \n",
    "        \n",
    "    def tune_parameters(self, X, y, clf, parameter_grid, scoring=None, cv=None):\n",
    "        \"\"\" Given X, y, a classifier and a parameter grid, \n",
    "        find the best parameters for the classifier and data using GridSearch\n",
    "        with cross validation.\n",
    "        \"\"\"\n",
    "        # The code below is from\n",
    "        # https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=self.random_state)\n",
    "\n",
    "        print(f\"# Tuning hyper-parameters for {scoring=}\")\n",
    "        print()\n",
    "\n",
    "        clf = GridSearchCV(     clf, \n",
    "                                parameter_grid, \n",
    "                                scoring=scoring,\n",
    "                                n_jobs=-1,\n",
    "                                cv=cv\n",
    "                            ).fit(X_train, y_train)\n",
    "\n",
    "        #piped_clf\n",
    "\n",
    "        print(\"Best parameters set found on development set:\")\n",
    "        print()\n",
    "        print(f\"{clf.best_params_}, score: {clf.best_score_:.4f}\")\n",
    "        print()\n",
    "        \"\"\"print(\"Grid scores on development set:\")\n",
    "        print()\n",
    "        means = clf.cv_results_['mean_test_score']\n",
    "        stds = clf.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\"\"\"\n",
    "\n",
    "        print(\"Classification report:\")\n",
    "        print()\n",
    "\n",
    "        print(classification_report(y_test, clf.predict(X_test)))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    def find_alpha(self, beta,p):\n",
    "        \"\"\" Given beta and a mean probability p, compute and return the alpha of a beta distribution. \"\"\"\n",
    "        return beta*p/(1-p)\n",
    "\n",
    "    def find_efficacy(self, group_pos: pd.DataFrame, group_neg: pd.DataFrame, symptom, prior_probs):\n",
    "        if isinstance(symptom, int):\n",
    "            symptom_index = symptom\n",
    "            symptom_name = group_pos.keys()[symptom]\n",
    "        else:\n",
    "            symptom_name = symptom\n",
    "            symptom_index = list(group_pos.keys()).index(symptom)\n",
    "        \n",
    "        group_pos_count = np.sum(group_pos[symptom_name] * group_pos.iloc[:,1])\n",
    "        group_neg_count = np.sum(group_neg[symptom_name] * group_neg.iloc[:,1])\n",
    "\n",
    "        v = group_pos_count/len(group_pos)\n",
    "        n_v = group_neg_count/len(group_neg)\n",
    "\n",
    "        if n_v == 0:\n",
    "            print(f'{v=}, {n_v=}: Division by zero')\n",
    "            return\n",
    "\n",
    "        IRR = v/n_v\n",
    "\n",
    "        #print(v, n_v)\n",
    "        efficacy = 100*(1- IRR)\n",
    "\n",
    "        N = 100_000\n",
    "        beta = 1\n",
    "        p = prior_probs[symptom_index]\n",
    "        alpha = self.find_alpha(beta,p)\n",
    "\n",
    "        samples_group_pos = stats.beta.rvs(alpha + group_pos_count, beta + len(group_pos) - group_pos_count, size=N)\n",
    "        samples_group_neg = stats.beta.rvs(alpha + group_neg_count, beta + len(group_neg) - group_neg_count, size=N)\n",
    "\n",
    "        samples_ve = 100 * (1 - samples_group_pos/samples_group_neg)\n",
    "        lower = np.percentile(samples_ve, 2.5)\n",
    "        upper = np.percentile(samples_ve, 97.5)\n",
    "        print(f'{symptom_name:15s}: {efficacy:3.3f} - ({lower:3.3f}, {upper:3.3f})')  \n",
    "        \n",
    "        \n",
    "    def side_effects(self, vacced_neg, un_vacced_neg, start, end):\n",
    "        df = pd.DataFrame(index=vacced_neg.keys()[start:end], \n",
    "                          columns = (\"p1 (%)\", \"p2 (%)\", \"Diff (%)\", \"Credible Interval (%)\", \"Null Hypothesis\", ),\n",
    "                         )\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            symptom = vacced_neg.keys()[i]\n",
    "            #print(vacced_neg.sum())\n",
    "            p1 = vacced_neg.sum()[symptom] / len(self.y) / (len(vacced_neg) / len(self.y))\n",
    "            p2 = un_vacced_neg.sum()[symptom] / len(self.y) / (len(un_vacced_neg) / len(self.y))\n",
    "            \n",
    "            \n",
    "            \n",
    "            lower = (p1-p2 - 1.64 * np.sqrt((p1*(1-p1) / len(vacced_neg)) + (p2 * (1-p2) / len(un_vacced_neg))))\n",
    "            higher = (p1-p2 + 1.64 * np.sqrt((p1*(1-p1) / len(vacced_neg)) + (p2 * (1-p2) / len(un_vacced_neg))))\n",
    "            \n",
    "            p1, p2 = p1 * 100, p2 * 100\n",
    "            lower, higher = lower * 100, higher * 100\n",
    "            \n",
    "            df.loc[symptom] = [round(p1, 4), round(p2, 4), round(p1 - p2, 4), (round(lower, 4), round(higher, 4)),\n",
    "                               \"rejected\" if lower>0 else \"not rejected\", ]\n",
    "                        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def treatment_efficacy(self, outcome_treated, precondition_treated, outcome_untreated, precondition_untreated, p, symptom_name):\n",
    "        group_pos_count = np.sum(outcome_treated[symptom_name])\n",
    "        group_neg_count = np.sum(outcome_untreated[symptom_name])\n",
    "\n",
    "        group_pos_total = np.sum(precondition_treated[symptom_name])\n",
    "        group_neg_total = np.sum(precondition_untreated[symptom_name])\n",
    "\n",
    "        if any(v == 0 for v in (group_pos_total, group_neg_total, group_neg_count)):\n",
    "            print(f'{symptom_name:15s}: Division by zero - not enough data to compute efficacy' )\n",
    "            return\n",
    "\n",
    "        v = group_pos_count / group_pos_total\n",
    "        n_v = group_neg_count / group_neg_total\n",
    "\n",
    "        #print(f\"{group_pos_count=}, {group_pos_total=}\\n{group_neg_count=} {group_neg_total=}\\n,{v=}, {n_v=}\")\n",
    "\n",
    "        IRR = v/n_v\n",
    "\n",
    "        efficacy = 100 * (1- IRR)\n",
    "\n",
    "        N = 100_000\n",
    "        beta = 1\n",
    "        p\n",
    "        alpha = self.find_alpha(beta,p)\n",
    "\n",
    "        #symptom_name = symptom_names[symptom_index]\n",
    "\n",
    "        samples_group_pos = stats.beta.rvs(alpha + group_pos_count, beta + len(outcome_treated) - group_pos_count, size=N)\n",
    "        samples_group_neg = stats.beta.rvs(alpha + group_neg_count, beta + len(outcome_untreated) - group_neg_count, size=N)\n",
    "\n",
    "        samples_ve = 100 * (1 - samples_group_pos/samples_group_neg)\n",
    "        lower = np.percentile(samples_ve, 2.5)\n",
    "        upper = np.percentile(samples_ve, 97.5)\n",
    "        print(f'{symptom_name:15s}: {efficacy:7.3f} - 95% CI: ({lower:3.3f}, {upper:3.3f})')    \n",
    "        \n",
    "        return efficacy, (lower, upper)\n",
    "\n",
    "    def bootstrap(self, X=None, y=None, N=None):\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        if y is None:\n",
    "            y = self.y\n",
    "            \n",
    "        return resample(X, y, n_samples=N)\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a1c744c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples with vaccinations or not:\n",
    "vacced = obs_data[np.sum(obs_data.iloc[:,-3:], axis=1) == 1]\n",
    "vacced_neg = vacced[vacced.iloc[:,1]==0]\n",
    "vacced_pos = vacced[vacced.iloc[:,1]==1]\n",
    "\n",
    "un_vacced = obs_data[np.sum(obs_data.iloc[:,-3:], axis=1) == 0]\n",
    "un_vacced_neg = un_vacced[un_vacced.iloc[:,1]==0]\n",
    "un_vacced_pos = un_vacced[un_vacced.iloc[:,1]==1]\n",
    "\n",
    "symptom_names = ['Covid-Recovered', 'Covid-Positive', 'No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', \n",
    "                 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c36ab19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = np.array(obs_data.iloc[:,0:10])\n",
    "age = obs_data.iloc[:,10]\n",
    "gender = obs_data.iloc[:,11]\n",
    "income = obs_data.iloc[:,12]\n",
    "genome = obs_data.iloc[:,13:141]\n",
    "comorbidities = obs_data.iloc[:,141:147]\n",
    "vaccination_status = obs_data.iloc[:,147:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a7097003",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(un_vacced_pos.iloc[:,13:147], un_vacced_pos['Blood-Clots'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e61c7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_example(pipeline, X=None, y=None, synthetic=True):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    parameter_grid = [{'kernel': ['poly', 'rbf'],\n",
    "                         'C': [0.01, 0.1,1, 10, 100,],\n",
    "                         'gamma': [.1, .01, 1e-3]}, ]\n",
    "    \n",
    "    if X and y:\n",
    "        pass\n",
    "    \n",
    "    # The generate binary data function gives some correlation by default to 1st and 2nd feature\n",
    "    elif synthetic is True:\n",
    "        X, y = pipe.generate_binary_data(100, 1000)\n",
    "    else:\n",
    "        X, y = pipe.X, pipe.y\n",
    "        \n",
    "    best_features = pipe.select_features(X, y, 0.8)\n",
    "    print(f\"{best_features=}\")\n",
    "    \n",
    "    keys = list(X.keys())\n",
    "    for f in best_features:\n",
    "        print(keys[f], end=' ')\n",
    "    print()\n",
    "    \n",
    "    print(\"Classification using best features\")\n",
    "    pipeline.tune_parameters(X.iloc[:, best_features], y, SVC(), parameter_grid=parameter_grid)\n",
    "\n",
    "    \n",
    "    print(\"Classification using all features\")    \n",
    "    pipeline.tune_parameters(X, y, SVC(), parameter_grid=parameter_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f805b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_features=[0, 1]\n",
      "0 1 \n",
      "Classification using best features\n",
      "# Tuning hyper-parameters for scoring=None\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}, score: 0.8507\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77        96\n",
      "           1       0.81      1.00      0.90       154\n",
      "\n",
      "    accuracy                           0.86       250\n",
      "   macro avg       0.91      0.81      0.83       250\n",
      "weighted avg       0.88      0.86      0.85       250\n",
      "\n",
      "\n",
      "Classification using all features\n",
      "# Tuning hyper-parameters for scoring=None\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}, score: 0.8467\n",
      "\n",
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80        96\n",
      "           1       0.87      0.88      0.87       154\n",
      "\n",
      "    accuracy                           0.84       250\n",
      "   macro avg       0.84      0.83      0.83       250\n",
      "weighted avg       0.84      0.84      0.84       250\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_features_example(pipe, synthetic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af679235",
   "metadata": {},
   "source": [
    "The results above show that using only the 2 features that the select_features function returned, gives the same or better results than using all 100 features of the synthetic dataset, when there is a high correlation between the target and those two features, and the other features are completely random.\n",
    "\n",
    "Because the given data is unbalanced, it is hard to see how the classifier using only the selected features works compared to one using all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "26dbf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_efficacy_example(pipeline, N=10000, p=1):\n",
    "    \"\"\" Create example data where the treatment is  about p effective, default 1,\n",
    "    to test the treatment_efficacy function\"\"\"\n",
    "    \n",
    "    X, y = pipeline.generate_binary_data(2, N, correlation =[0, 0.1])\n",
    "    X.columns = [0, 'Target']\n",
    "    \n",
    "    # Need to set y 'manually' here\n",
    "    y = (np.random.choice(2, size=N, p=(1-p, p)) * X[0] ^ 1) * X.Target * np.random.choice(2, size=N, p=(1-p, p))\n",
    "    y = pd.DataFrame(y, columns=['Target'])\n",
    "    \n",
    "    prior_prob = np.sum(X.Target) / len(y)\n",
    "    \n",
    "        # Partition X and y into the groups where treatment is given or not given, and test efficacy\n",
    "    pipeline.treatment_efficacy(y[X[0] == 1], X[X[0] == 1], y[X[0] == 0], X[X[0] == 0], prior_prob, 'Target')\n",
    "    \n",
    "    return X.assign(y=y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9a6ec098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaccine_efficacy_example(pipeline, N=10000, p=0.9):\n",
    "    \"\"\" Here p is the chance that a vaccinated person does not get symptom\n",
    "    and that a non-vaccinated person does get the symptom.\n",
    "    p = 0.5 would mean no effect, as both groups would be equally susceptible\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = pipeline.generate_binary_data(1, N, correlation =[1-p])\n",
    "    \n",
    "    # Need to set y 'manually' here as well, \n",
    "    # here by 'flipping' a random amount of vaccination bits\n",
    "    y = (np.random.choice(2, size=N, p=(1-p, p)) ^ X[0])\n",
    "    X = X.assign(Target=y)\n",
    "    \n",
    "    prior_probs = [0, np.sum(X.Target) / len(y)]\n",
    "    \n",
    "    # Partition X and y into vaccinated and non-vaccinated groups\n",
    "    pipeline.find_efficacy(X[X[0] == 1], X[X[0] == 0], 'Target', prior_probs)\n",
    "    \n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "84eacf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target         : -1.149 - (-5.304, 2.861)\n"
     ]
    }
   ],
   "source": [
    "X = vaccine_efficacy_example(pipe, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf92a54",
   "metadata": {},
   "source": [
    "## 1b) Estimating the efficacy of vaccines\n",
    "\n",
    "We estimate the efficacy of the vaccines by\n",
    "$$ 100 \\cdot (1-IRR)$$\n",
    "where IRR is the ratio of the vaccinated incidence rate and not vaccinated incidence incidenes ([link](https://www.nejm.org/doi/full/10.1056/NEJMoa2034577)). This means that the relationship we model for each symptom is as given below:\n",
    "\n",
    "$r_v = P(symptom = 1 | vaccine = 1)$ and\n",
    "\n",
    "$r_n = P(symptom = 1 | vaccine = 0)$\n",
    "\n",
    "to obtain $IRR = 100 * [1 - r_v / r_n]$\n",
    "\n",
    "We obtain a Bayesian estimate of $r_v, r_n$\n",
    "through the Beta-Bernoulli conjugate prior.\n",
    "\n",
    "As in the abovementioned article, we use a Bayesian Beta-Binomial model. Hence, our prior belief is assumed to have a Beta distribution and the Likelihood of observed data is Binomial distributed. This results in a Beta-distributed posterior (more details will follow afterwards).\n",
    "\n",
    "We set a low beta value for the prior distribution because we are uncertain about the actual distribution and compute a mean prior probability of each symptom from the data. Given the beta value of 1 and the mean probability, we compute the alpha value using the function alpha = beta \\* p/(1-p), which for p in (0, 0.5] returns an alpha in (0, 1]. If p is 0.5 this would make the beta distribution uniform, with alpha = beta = 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "6c9e3d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficacy of the vaccinated:\n",
      "Covid-Recovered: 28.965 - (20.046, 36.907)\n",
      "Covid-Positive : 21.464 - (19.616, 23.267)\n",
      "No-Taste/Smell : 47.521 - (41.639, 52.826)\n",
      "Fever          : 51.575 - (44.928, 57.437)\n",
      "Headache       : 45.473 - (28.912, 58.399)\n",
      "Pneumonia      : 57.810 - (51.542, 63.304)\n",
      "Stomach        : 50.542 - (26.085, 67.338)\n",
      "Myocarditis    : 44.531 - (28.545, 57.119)\n",
      "Blood-Clots    : 57.190 - (50.861, 62.790)\n",
      "Death          : 93.394 - (90.537, 95.677)\n"
     ]
    }
   ],
   "source": [
    "prior_probs= [np.sum(obs_data.iloc[:,i]) / len(obs_data) for i, key in enumerate(symptom_names)]\n",
    "\n",
    "print(\"Efficacy of the vaccinated:\")\n",
    "for i, s in enumerate(symptom_names):\n",
    "    pipe.find_efficacy(vacced,un_vacced,i,prior_probs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d81522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficacy for Vacc_1\n",
      "Covid-Recovered: 19.633 - (6.145, 31.504)\n",
      "Covid-Positive : 11.296 - (8.526, 14.016)\n",
      "No-Taste/Smell : 34.419 - (24.614, 43.321)\n",
      "Fever          : 43.590 - (32.769, 53.145)\n",
      "Headache       : 30.767 - (2.793, 52.303)\n",
      "Pneumonia      : 54.129 - (44.197, 62.743)\n",
      "Stomach        : 28.704 - (-16.934, 59.471)\n",
      "Myocarditis    : 36.093 - (10.324, 55.787)\n",
      "Blood-Clots    : 48.746 - (38.220, 57.968)\n",
      "Death          : 95.704 - (91.539, 98.436)\n",
      "\n",
      "Efficacy for Vacc_2\n",
      "Covid-Recovered: 28.837 - (16.286, 39.895)\n",
      "Covid-Positive : 20.584 - (17.983, 23.134)\n",
      "No-Taste/Smell : 43.109 - (33.994, 51.189)\n",
      "Fever          : 45.703 - (35.206, 54.965)\n",
      "Headache       : 39.907 - (14.333, 59.394)\n",
      "Pneumonia      : 48.345 - (37.884, 57.595)\n",
      "Stomach        : 70.324 - (42.831, 87.536)\n",
      "Myocarditis    : 35.284 - (9.514, 55.000)\n",
      "Blood-Clots    : 47.762 - (37.167, 57.047)\n",
      "Death          : 92.922 - (87.769, 96.638)\n",
      "\n",
      "Efficacy for Vacc_3\n",
      "Covid-Recovered: 38.211 - (26.770, 48.229)\n",
      "Covid-Positive : 32.271 - (29.905, 34.589)\n",
      "No-Taste/Smell : 64.686 - (57.855, 70.766)\n",
      "Fever          : 65.183 - (57.098, 72.157)\n",
      "Headache       : 65.346 - (46.611, 79.133)\n",
      "Pneumonia      : 70.765 - (63.229, 77.340)\n",
      "Stomach        : 52.328 - (16.650, 75.722)\n",
      "Myocarditis    : 61.919 - (43.286, 76.017)\n",
      "Blood-Clots    : 74.762 - (67.763, 80.789)\n",
      "Death          : 91.603 - (86.011, 95.697)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for vacc in vaccination_status:\n",
    "    print(f\"Efficacy for {vacc}\")\n",
    "    for i, s in enumerate(symptom_names):\n",
    "        pipe.find_efficacy(obs_data[obs_data[vacc] == 1],un_vacced,i,prior_probs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e5d6d",
   "metadata": {},
   "source": [
    "## 1c) Estimating the probability of vaccination side-effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed7484",
   "metadata": {},
   "source": [
    "Using the Bayes theorem, we will estimate the probability of vaccination side-effects. In other words, the probability of getting symptoms given that a person is vaccinated and has tested negative.\n",
    "$$p_1 = P(symptom | vaccine,covid') = \\frac{P(symptom \\cap vaccine,covid')}{P(vaccine,covid')} $$\n",
    "$$p_2 = P(symptom | vaccine',covid') = \\frac{P(symptom \\cap vaccine',covid')}{P(vaccine',covid')} $$\n",
    "\n",
    "We want to test the following hypothesis:\n",
    "$$ h_0: p_1 - p_2 \\leq 0 $$\n",
    "$$h_a: p_1 - p_2 > 0 $$\n",
    "\n",
    "We make a confidence intervall for each symptom, and reject the null hypothesis if confidence intervall does not include zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eae2f8",
   "metadata": {},
   "source": [
    "**A large-sample 95% confidence interval for $p_1 - p_2$**:\n",
    "\n",
    "In this task it seems appropiate to estimate confidence intervals instead of doing the task of comparison through hypothesis testing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The variables X and Y  represent the number of individuals in each sample having a certain  characteristic that defines $p_1$ and $p_2$.Provided the population sizes are much larger than the sample sizes, the distribution\n",
    "of X can be choosen to be binomial with parameters m and $p_1$, and similarly, Y is choosen\n",
    "to be a binomial variable with parameters n and $p_2$. The samples are assumed to be independent of each other.Therefore X and Y are independent rvâ€™s.\n",
    "\n",
    "The estimator for $ p_1-p_2 $, the difference in population proportions, is\n",
    "the  difference in sample proportions $\\frac{X}{m} - \\frac{Y}{n}$ .  $ \\hat{p_1} = \\frac{X}{m}$ and\n",
    "$\\hat{p_2} = \\frac{Y}{m}$, the estimator of $ p_1-p_2 $ can be expressed as $\\hat{p_1} - \\hat{p_2}$.\n",
    "\n",
    "$E[\\hat{p_1} - \\hat{p_2}]=p_1-p_2$ so $\\hat{p_1} - \\hat{p_2}$ is an unbiased estimate of $p_1-p_2$ \n",
    "\n",
    "$\\hat{p_1}=\\frac{X}{m}$ and $\\hat{p_2}=\\frac{y}{n}$ are aproximately normal distributed when m and n are large.\n",
    "\n",
    "\n",
    "Refrence: Modern Mathematical Statistic with applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "43afe30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaccine_side_effect_example(pipeline, N=10000, p=0.9):\n",
    "    \"\"\" Here p is the probability of vaccinated people to get the symptom\n",
    "    1 - p is the probability for non_vaccinated people to get the symptom\n",
    "    p = 0.5 would mean no effect here, as both groups would be equally susceptible.\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = pipeline.generate_binary_data(1, N)\n",
    "    \n",
    "    y = X[0] ^ np.random.choice(2, size=N, p=(p, 1-p))\n",
    "    \n",
    "    # target y needs to be partof the same dataframe for this funcion to work\n",
    "    X = X.assign(Target=y)\n",
    "    \n",
    "    prior_probs = [0, np.sum(X.Target) / len(y)]\n",
    "    \n",
    "    # Partition X and y into vaccinated and non-vaccinated groups\n",
    "    return pipeline.side_effects(X[X[0] == 1], X[X[0] == 0], 1, 2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a80fd81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1 (%)</th>\n",
       "      <th>p2 (%)</th>\n",
       "      <th>Diff (%)</th>\n",
       "      <th>Credible Interval (%)</th>\n",
       "      <th>Null Hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>No-Taste/Smell</th>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>(-0.0067, 0.0554)</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fever</th>\n",
       "      <td>9.9028</td>\n",
       "      <td>0.5081</td>\n",
       "      <td>9.3947</td>\n",
       "      <td>(9.1613, 9.6281)</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headache</th>\n",
       "      <td>5.5788</td>\n",
       "      <td>1.053</td>\n",
       "      <td>4.5258</td>\n",
       "      <td>(4.3287, 4.7229)</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumonia</th>\n",
       "      <td>0.1332</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>-0.0106</td>\n",
       "      <td>(-0.0557, 0.0345)</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stomach</th>\n",
       "      <td>0.231</td>\n",
       "      <td>0.2574</td>\n",
       "      <td>-0.0264</td>\n",
       "      <td>(-0.0864, 0.0336)</td>\n",
       "      <td>not rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myocarditis</th>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>(0.1273, 0.2077)</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blood-Clots</th>\n",
       "      <td>0.2435</td>\n",
       "      <td>0.1103</td>\n",
       "      <td>0.1331</td>\n",
       "      <td>(0.0847, 0.1816)</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Death</th>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>(0.0213, 0.0494)</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                p1 (%)  p2 (%) Diff (%) Credible Interval (%) Null Hypothesis\n",
       "No-Taste/Smell  0.0812  0.0568   0.0243     (-0.0067, 0.0554)    not rejected\n",
       "Fever           9.9028  0.5081   9.3947      (9.1613, 9.6281)        rejected\n",
       "Headache        5.5788   1.053   4.5258      (4.3287, 4.7229)        rejected\n",
       "Pneumonia       0.1332  0.1437  -0.0106     (-0.0557, 0.0345)    not rejected\n",
       "Stomach          0.231  0.2574  -0.0264     (-0.0864, 0.0336)    not rejected\n",
       "Myocarditis     0.2143  0.0468   0.1675      (0.1273, 0.2077)        rejected\n",
       "Blood-Clots     0.2435  0.1103   0.1331      (0.0847, 0.1816)        rejected\n",
       "Death           0.0354     0.0   0.0354      (0.0213, 0.0494)        rejected"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.side_effects(vacced_neg, un_vacced_neg, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "547c9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1 (%)</th>\n",
       "      <th>p2 (%)</th>\n",
       "      <th>Diff (%)</th>\n",
       "      <th>Credible Interval (%)</th>\n",
       "      <th>Null Hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>60.3512</td>\n",
       "      <td>40.4743</td>\n",
       "      <td>19.8768</td>\n",
       "      <td>(18.2693, 21.4843)</td>\n",
       "      <td>rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         p1 (%)   p2 (%) Diff (%) Credible Interval (%) Null Hypothesis\n",
       "Target  60.3512  40.4743  19.8768    (18.2693, 21.4843)        rejected"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaccine_side_effect_example(pipe, p=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f497a18",
   "metadata": {},
   "source": [
    "# 2)\n",
    "The relation we are modelling here is whether any of the individuals who had symptoms do not have those symptoms after treatment or got symptoms from the treatment. \n",
    "\n",
    "We are not interested in the outcomes for individuals who did not have any symptoms neither before nor after the treatment.\n",
    "\n",
    "We do this modelling task in a similar way that we did the modelling for vaccine efficacy. Since we now have data from before and after a treatment, we calculate the ratio of people who still have symptoms after treatment to the people who had symptoms before treatment for both the treated group and the control group. \n",
    "\n",
    "\n",
    "We estimate the efficacy of the treatments by\n",
    "$$ 100 \\cdot (1-IRR)$$\n",
    "where IRR is the ratio of the treated incidence rate $r_t$ and not treated incidence rate $r_n$ ([link](https://www.nejm.org/doi/full/10.1056/NEJMoa2034577)). This means that the relationship we model for each symptom is as given below:\n",
    "\n",
    "$t_1 = P(symptom = 1 | treatment = 1)$ after treatment, $t_0 = P(symptom = 1 | treatment = 1) $ before treatment.\n",
    "\n",
    "$n_1 = P(symptom = 1 | treatment = 0)$ after treatment, $n_0 = P(symptom = 1 | treatment = 0) $ before treatment\n",
    "\n",
    "$r_t = t_1 / t_0$ and\n",
    "\n",
    "$r_n = n_1 / n_0$\n",
    "\n",
    "to obtain $IRR = 100 * [1 - r_t / r_n]$\n",
    "\n",
    "We want to test the following hypothesis:\n",
    "$$ h_0: IRR \\leq 0 $$\n",
    "$$h_a: IRR > 0 $$\n",
    "\n",
    "We make a confidence intervall for each symptom, and reject the null hypothesis if confidence intervall does not include zero. Since the method we are using is used for calculation of efficacy, the lower bound of the confidence interval can be an arbitrarily big negative number, when the ratio $ r_t / r_n # approaches infinity. Because we only want to predict the effects that are statistically significant, we ignore the results with such a lower bound, only asserting that the treatment has no statistically  significant effect on that symptom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4ccac94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_treat_data = treat_data[((np.sum(treat_data.iloc[:,2:10],axis=1) > 0.0) \n",
    "              | np.sum(outcome_data.iloc[:,2:10],axis=1) > 0.0)]\n",
    "\n",
    "group_first = new_treat_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 0))]\n",
    "group_second = new_treat_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 1))]\n",
    "group_both = new_treat_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 1))]\n",
    "group_none = new_treat_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "b4f665fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outcome_data = outcome_data[((np.sum(treat_data.iloc[:,2:10],axis=1) > 0.0) \n",
    "              | np.sum(outcome_data.iloc[:,2:10],axis=1) > 0.0)]\n",
    "\n",
    "outcome_first = new_outcome_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 0))]\n",
    "outcome_second = new_outcome_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 1))]\n",
    "outcome_both = new_outcome_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 1))]\n",
    "outcome_none = new_outcome_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "6ef8ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior probs as the average of the chance of people having the \n",
    "# symptom before and after the treatment, ignoring covid-recovered and covid-positive columns\n",
    "prior_probs= [(np.sum(new_treat_data[sym]) + np.sum(new_outcome_data[sym])) / (len(new_treat_data) * 2) \n",
    "              for sym in symptom_names][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "94f4d17c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment 1 efficacy:\n",
      "No-Taste/Smell :  33.333 - 95% CI: (-17.813, 75.314)\n",
      "Fever          :  40.000 - 95% CI: (-117.779, 73.215)\n",
      "Headache       : 100.000 - 95% CI: (48.375, 100.000)\n",
      "Pneumonia      :  80.000 - 95% CI: (29.237, 96.956)\n",
      "Stomach        : Division by zero - not enough data to compute efficacy\n",
      "Myocarditis    :   0.000 - 95% CI: (-11253.097, 37.279)\n",
      "Blood-Clots    : 100.000 - 95% CI: (85.844, 100.000)\n",
      "Death          :   0.000 - 95% CI: (-1375.168, 61.952)\n",
      "\n",
      "treatment 2 efficacy:\n",
      "No-Taste/Smell :  71.429 - 95% CI: (50.873, 97.511)\n",
      "Fever          :  25.000 - 95% CI: (-36.948, 91.064)\n",
      "Headache       : 100.000 - 95% CI: (48.640, 100.000)\n",
      "Pneumonia      :   0.000 - 95% CI: (-105.031, 64.221)\n",
      "Stomach        :  33.333 - 95% CI: (-6585.234, 80.943)\n",
      "Myocarditis    :  75.000 - 95% CI: (-3088.415, 97.036)\n",
      "Blood-Clots    :  46.154 - 95% CI: (-59.909, 72.812)\n",
      "Death          :  50.000 - 95% CI: (-390.990, 98.446)\n",
      "\n",
      "both treatments efficacy:\n",
      "No-Taste/Smell : 100.000 - 95% CI: (84.944, 100.000)\n",
      "Fever          :  33.333 - 95% CI: (-8.375, 95.822)\n",
      "Headache       : 100.000 - 95% CI: (42.142, 100.000)\n",
      "Pneumonia      : 100.000 - 95% CI: (81.679, 100.000)\n",
      "Stomach        : 100.000 - 95% CI: (41.704, 100.000)\n",
      "Myocarditis    : 100.000 - 95% CI: (-73.080, 100.000)\n",
      "Blood-Clots    : 100.000 - 95% CI: (84.369, 100.000)\n",
      "Death          :  50.000 - 95% CI: (-418.751, 98.416)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for outcome_treated, pre_treated, treatment in zip([outcome_first, outcome_second, outcome_both], \n",
    "                                                [group_first, group_second, group_both],\n",
    "                                                ['treatment 1', 'treatment 2', 'both treatments']):\n",
    "    print(f\"{treatment} efficacy:\")\n",
    "    for i, key in enumerate(outcome_data.keys()[2:]):\n",
    "        pipe.treatment_efficacy(outcome_treated, pre_treated, outcome_none, group_none, prior_probs[i], key)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "73d3a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target         :  90.639 - 95% CI: (89.242, 91.769)\n"
     ]
    }
   ],
   "source": [
    "t = treatment_efficacy_example(pipe, p=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57b19f",
   "metadata": {},
   "source": [
    "# 3)\n",
    "\n",
    "3. Although this data involves people, you are not required to do a formal study of fairness or privacy in this part of the project. However, you are encouraged to describe verbally what possible issues are.\n",
    "\n",
    "These data are health data and genomic data which are classified as sensitive personal data and have special protection under the law (https://lovdata.no/dokument/NL/lov/2018-06-15-38/KAPITTEL_gdpr-2#gdpr/a9). It is not allowed to use such data unless some conditions are satisfied, such as prior consent from the people the data regards, or specifically for this case that use of the data is necessary for public health concerns. Such necessary use does still require that special measures are taken to ensure that the rights of the registered individuals are protected.\n",
    "\n",
    "The data we have is very precise, e.g. we have the age and income as decimal numbers with enough precision that they can be used to identify the people the data regards. One way to help rectify this could be to divide the data into age and income groups in a way that gives some k-anonymity with regards to these identifiers. However, considering we have 128 bits of the genome for each person, it is highly unlikely that we can give more than 1-anonymity without dropping this part of the data set.\n",
    "\n",
    "When it comes to the actual output of our research, it is, however, hard to infer from this output much about the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a5fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
