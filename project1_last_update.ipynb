{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9735eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d90a2e",
   "metadata": {},
   "source": [
    "# Project 1: Data analysis\n",
    "\n",
    "## Bayesian approch\n",
    "\n",
    "In order to predict the effect of genes on symptoms, we will use a simple Bayesian model. We consider these hypothesis:\n",
    "\n",
    "$$\\mu_0: P(symptoms|genes) = P(symptoms)$$\n",
    "$$\\mu_1: P(symptoms|genes) \\neq P(symptoms)$$\n",
    "\n",
    "In order words, does the genes have any effect on the symptoms or not? If it does, which genomes are the most relevant for the preventing the symptoms? \n",
    "\n",
    "\n",
    "We let Y be the symptoms and our X the genes. Y is an 8 bit vector for each observation,with 1 or zero for each bit. X is an 128 with the same charectaristic as Y.\n",
    "\n",
    "\n",
    "The probability distribution of $ Y $ depends on an unknown parameter that we assume to be stochastic. We call this unknown parameter $\\theta$ or $ p $. \n",
    "\n",
    "The formulation under is an alternative formulation of the the previous hypotesis. Does the parameter that the probability distribution of Y depend on, depend on X or not. $ \\mu_0 $, our null, says that it does not. $\\mu_1 $, the alternative hypothesis claims it does. We assume the Beta-Bernoulli model:\n",
    " \n",
    "$$\\mu_0: \\theta^0 \\sim Beta(\\alpha^0 , \\beta^0) ,  \\ \\ \\  Y | \\theta^0  $$\n",
    "$$\\mu_1: \\theta^{1,x_t} \\sim Beta(\\alpha^{1,x_t} , \\beta^{1,x_t}) , \\ \\ \\ y_t | x_t \\sim bernoulli(\\theta^{1,x_t}) $$\n",
    " \n",
    "\n",
    "Given the data, D, we try to estimate\n",
    "\n",
    "$$ P(\\mu | D) = \\frac{P_{\\mu} (D) \\cdot \\xi(\\mu)}{\\sum_{i=1} ^n P_{\\mu_1}(D) \\cdot \\xi(\\mu_1)} $$\n",
    "\n",
    "We can either set $P(\\mu | D)$ to mean $P(\\mu_1 | D)$ or  $P(\\mu_0 | D)$ depending on which hypothesis we want to check the posterior belief of. \n",
    "\n",
    "We let $P_{\\mu} (D) = P(\\mu |D)$, $D = (x_t, y_t,x_{t-1},y_{t-1})_{t=1} ^T $ for a spesific gene, and let $x_t$ be the value of the gene for observation t (0  or 1), $x_{t-1}$ is a vector of all the previous values of the previous observation up to t-1,  $y_t$ is the the value of symptom Y for observation t (0 or 1),$y_{t-1}$ is a vector of all the previous values of the previous observations including t-1 and $\\mu$ is one of the hypothesis in the set {$\\mu_1$,$\\mu_0$}.  We assume that the different observations(rows) of the data are independent of each other. This gives us:\n",
    "$$ P_{\\mu}(D) = P(y_1,...,y_t \\cup x_1,...,x_t)$$\n",
    "\n",
    "$$ = \\prod_{i=1}^{t} P(y_i | x_i,x_{i-1} , y_{i-1}) $$\n",
    "\n",
    "\n",
    "With these assumptions, we can estimate the terms in the $P_{\\mu} (D)$ by \n",
    "$$ P(y_t | x_t,x_{t-1} , y_{t-1}) = \\int_{\\theta} P_{\\theta}(y_t|x_t) \\cdot \\xi(\\theta|x_{t-1},y_{t-1})$$\n",
    "\n",
    "We calculate the likehood of adding a new observation(row) using this formula. It works if we assume that the the next observation is independent from the previous observations.\n",
    "\n",
    "\n",
    "This is the marginal likelihood(posterior predictive distribution) which is a compound distribution,which is used to calculate $P(y_t | x_{t},x_{t-1}, y_{t-1})$. The probabilty $P(y_t | x_{t})$ is stochastic in $\\theta$, so we have to integrate over all values of $\\theta$. Every time a new observation is added we calculate $P(y_t | x_{t},x_{t-1}, y_{t-1})$,and recalculate $$ = \\prod_{i=1}^{t} P(y_i | x_i ,x_{i-1}, y_{i-1}) $$. \n",
    "\n",
    "So to calculate $$ P(\\mu | D) = \\frac{P_{\\mu} (D) \\cdot \\xi(\\mu)}{\\sum_{i=1} ^n P_{\\mu_i}(D) \\cdot \\xi(\\mu_i)} $$, we are sequentially updating our belief every time a new observation is added. For every update $\\xi(\\mu)$, $P(\\mu | D)$ is from the previous iteration, exept for the first one. $\\xi(\\mu)$ will cancel because it's constant.\n",
    "\n",
    "This integral simplifies to:\n",
    "\n",
    "$$ P(y_t | x_{t},x_{t-1},y^{t-1}) = \\int_{\\theta} P_{\\theta}(y_t) \\cdot d \\xi(\\theta|y^{t-1}) = \\frac{\\alpha_{t,x_t}}{\\alpha_{t,x_t} + \\beta_{t,x_t}}$$\n",
    "\n",
    "\n",
    "Looking at $\\mu_1$ we are using the Beta-Bernoulli distribution. Its a binomial distribution where the probability of success at each of n trials is not fixed but comes from a Beta distribution. The Bernoulli distribtuion is a special case of the binomial distribution, where number of trials is equal to 1.\n",
    "\n",
    "We are doing this because our y's are either 1 or 0. It is then reasonable to assume that $Y|X$ is Bernoulli distributed.\n",
    "\n",
    "It is also conviniant because of its conjugate prior property. Now we can calculate $P( D | \\mu_1) = \\prod_{i=1}^{t} P(y_i | x^i ,x^{i-1}, y^{i-1}) $ beacuse we know that the marginal likelihood (posterior predictive dist) is composed of a Bernoulli distribution and a Beta dist $ P(y_t | x^t , y^{t-1}) = \\int_{\\theta} P_{\\theta}(y_t|x_t) \\cdot \\xi(\\theta|x^{t-1},y^{t-1})$, which is beta distributed. For each iteration we end up getting that $P_{\\mu} (D)$ is Beta distributed.\n",
    "\n",
    "The same integral calculation is done for $P(D | \\mu_0)$ but now the marginal likelihood is independent of the data.\n",
    "\n",
    "Instead of calculating $P(D |\\mu_1) = \\prod_{i=1}^{t} P(y_i | x_i ,x_{i-1}, y_{i-1})$, we can first calculate $log(P(  D|\\mu_1))$ and then do $e^{log(P(D|\\mu_1))}$,same for $P(D | \\mu_0)$.\n",
    "\n",
    "To summarize, we have a set of hypotesis. In this case two. We want to find how accurate they are, so we have a prior beleif on the strength of the hypothses which we express through a probability. We then want to update our belief of the hypotesis (posterior belief). In other words what is the probability of the choosen hypotesis given the data. We only have to compute one of the probabilities because $P(\\mu_1)= 1 - P(\\mu_0)$.\n",
    "\n",
    "The calculation is done sequentially. For each iteration we get a new data observation.A new row of genes.We continously update the posterior belief,for all genes,and put the posterior belief of each gene into a list. The belief get updated through calculating the marginal likelihood every time a new row is added and a product is taken between this marginal likelihood and all the presious marginal likelihood calculations bought for $P(  D|\\mu_1)$ and for$P(  D|\\mu_0)$ this is then put into the bayes formula $ P(\\mu | D) = \\frac{P_{\\mu} (D) \\cdot \\xi(\\mu)}{\\sum_{i=1} ^n P_{\\mu_i}(D) \\cdot \\xi(\\mu_i)} $.\n",
    "\n",
    "We want to find this probability for for each of the genes.This procedure is done for each gene.For each gene if the posterior probabilty is greater then some trechhold we add this gene to the list of important genes that are relvant for explaining that spesific symptom.\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "The decision rule can be defined as\n",
    "\n",
    "$$ \\mu =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\mu_1 & P(\\mu_1|D) > s \\\\\n",
    "      \\mu_0 & else\n",
    "\\end{array} \n",
    "\\right.  $$\n",
    "\n",
    "where $s$ is a given threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265a36e",
   "metadata": {},
   "source": [
    "Ones we have the important fetures we want to look at wether these fetures contribute negativly or positivley.We tried to do that through computing $P(symptom=1|gene=1)$.If this probability is greater then $P(symptom=1|gene=0)=1-P(symptom=1|gene=1)$ then the effect should be negative. \n",
    "\n",
    "We calculated $P(symptom|gene=1)$ the bayesian way,like in 1.a. We start with i prior uniform distrbution $\\alpha=1$ $\\beta=1$. Update $p=\\frac{\\alpha}{\\alpha+\\beta}$.If $symptom=1$ we update $\\beta$ if $symptom=0$ we update $\\alpha$.\n",
    "\n",
    "From 1.b we know that the posterior befelif of $P(symptom|gene=1)$ is given by $Beta(\\alpha_{prior}+k,\\beta_{prior}+n-k)$.\n",
    "\n",
    "$k=0 , n=0$ becomes the prior dist.We update k, when we want to update $\\alpha$ and $\\beta$. The gene is constantly set to one,which means we are only looking at people have that gene.\n",
    "\n",
    "\n",
    "These probabilities where calculated for all the relevant genes.P(symptom=1|gene=1) was around 1.5% for these genes,which could have ment that these genes have a positive effect.When we tested features that where not selected we got arouund the same procentage.The conclution is that one gene in itself have no effect on someone getting a symptom or not.\n",
    "\n",
    "There might be such that a combination of genes together could have a postive or negative effect.This is much harder to find out,since we assume that in the data set there is not a lot of people with the same sequence of genes.\n",
    "\n",
    "The select feature also tests $ P(symptoms|genes) \\neq P(symptoms)$,separatly for each gene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de666c2",
   "metadata": {},
   "source": [
    "The generate binary_data function gets the inputs :N number columns,num_feutures of rows and a correlation vector.These numbers are put into the numpys random.choice functon which creats a matrix wich has dimensions N x num_feutures.Each element is eather 0 or one.This matrix is then used to creat a correlation between each column in the matrix and the target vector. The target vector is set to be a 0 vector. Each element in the target vector is flipped based  on what the value the product between a single feature vector and the random.choice is, which is 1 with p=corr and 0 with p=1-cor. Based on which feature that changes the value of the target to 1, in position i,there is now a correlation between that position in the target vector and feuture i.\n",
    "Or = is used because $(1 \\cup 1)=1, \\ \\ (1 \\cup 0)=1,(0 \\cup 1)=1,(0 \\cup 0)=0$.  The downside with this procedure is that if one creat correlation between to many features most of the elements in the target becomes 1.\n",
    "\n",
    "\n",
    "We Want to test wether the methods we use in problem 2 works as expected.To that we generate data which reflects the assumption,that if you are treated with a treatment and you had symptoms before the treatment,then after the treatment you have none. \n",
    "We therefore use the $\\oplus$ operator: $$1  \\  \\ \\ \\oplus \\  \\  1 \\ \\ =0$$   \n",
    "$$ = symptom before=1  \\  \\  \\oplus  \\  \\ treated = symptoms \\; after=1$$\n",
    "\n",
    " $$0 \\  \\  \\  \\oplus \\  \\  \\ 1= \\  \\  1$$,   $$= (symptom before=0  \\  \\  \\ \\oplus \\  \\  \\ treated =1) = (symptoms after=1)$$\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062feae",
   "metadata": {},
   "source": [
    "## Estimating the efficacy of vaccines\n",
    "\n",
    "We estimate the efficacy of the vaccines by\n",
    "$$ 100 \\cdot (1-IRR)$$\n",
    "where IRR is the ratio of the vaccinated incidence rate and not vaccinated incidence incidenes ([link](https://www.nejm.org/doi/full/10.1056/NEJMoa2034577)). This means that the relationship we model for each symptom is as given below:\n",
    "\n",
    "$r_v = P(symptom = 1 | vaccine = 1)$ and\n",
    "\n",
    "$r_n = P(symptom = 1 | vaccine = 0)$\n",
    "\n",
    "to obtain $IRR = 100 * [1 - r_v / r_n]$\n",
    "\n",
    "Our prior belief is that r_v and r_n is equal,for all symptoms.Before we have seen the data we assume that wether you have any sumptom or not is independent from wether you have taken the vaccine or not. Theirfore we assume that these probabilities are beta distributed with the same $\\alpha$ and $\\beta$ parameter.To estimate these parameters we firstfind the frequency of people having a certain symptom for both the vaccinated group and the non vaccinated group and devide these frequency by the total number of observation.This is know our estimate of the expected value for r_v and r_n.So we have.\n",
    "\n",
    "$$V=frequency \\ \\ \\ \\ of \\  \\ people \\ \\ having \\ \\ a \\ \\ certain \\ \\ symptom \\ \\ for \\ \\ the \\ \\ vaccinated$$\n",
    "$$NV=frequency \\ \\ of \\ \\ people \\ \\  having \\ \\  a  \\ \\ certain \\ \\  symptom \\ \\ for \\ \\  none \\ \\  vaccinated$$\n",
    "$$T=Total \\ \\ number \\ \\ of \\ \\ observations$$\n",
    "\n",
    "$$E[r_v]=\\frac{\\alpha}{\\alpha+\\beta}=\\frac{V}{T} $$ \n",
    "$$E[r_n]=\\frac{\\alpha}{\\alpha+\\beta}=\\frac{NV}{T}$$\n",
    "\n",
    "Our prior belief is that $p=\\frac{V}{T}=\\frac{NV}{T}$.\n",
    "\n",
    "So:\n",
    "\n",
    "$$E[r_v]=E[r_n]=\\frac{\\alpha}{\\alpha+\\beta}=\\frac{V}{T} $$ \n",
    "\n",
    "\n",
    "The equation to solve in order to get the  $\\alpha$ and the $\\beta$ values for our prior belief is for example:\n",
    "\n",
    "$$\\frac{\\alpha}{\\alpha+\\beta}=\\frac{V}{T} $$\n",
    "\n",
    "Setting $\\beta=1$ we get $\\alpha=\\frac{\\beta*p}{1-p}$.\n",
    "\n",
    "we set $ \\frac{V}{T}= 0.02 $ as our guess before seening the data. probability 0.02 for getting a symptom, indepedent of what symptom. We set beta to be 1 and $ \\alpha<1$ beacause this distribution will have most of its mass around 0. In other words our prior belief is that the random variables r_v and r_n takes a value around 0 with a large probability. The probability of getting a symptom independent of vaccinated or not is very  likely to be around 0. When we get the data we can easly be proven wrong. The choosen posterior distrbution will also confurm that 0.02 is a good choice.\n",
    "\n",
    "After we get the data 0.02 could be a horrible choice(have a low probability in the prosterior dist),and $\\frac{V}{T}\\neq \\frac{NV}{T}$.\n",
    "\n",
    "We have that $Posterior \\propto Likelihood * Prior$.We are asuming that the prior comes from a beta distribution. The likelihhood is the likelihood of observing all the combinations of the observations.Our observations are sequences of 1 and 0's.Our likelihood is theirfore binomial dist.This makes the posterior beta dist.The prior and the posterior has the same dist.\n",
    "\n",
    "\n",
    "\n",
    "$Posterior \\propto Likelihood * Prior={n \\choose k}\\theta^k(1-\\theta)^{n-k} \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} \\propto \\theta^{\\alpha+k-1}(1-\\theta)^{\\beta+n-k-1}$ which is beta dist.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We use these posterior parameters to genrate N numbers of random variables,both for the vaccinated group and the none vaccinated group(monte carlo),in order to find a 95% credible intterval for our estimated efficacy of the vaccine on all the symptoms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf85ef4",
   "metadata": {},
   "source": [
    "## Estimating the probability of vaccination side-effects.\n",
    "\n",
    "Using the Bayes theorem, we will estimate the probability of vaccination side-effects. In other words, the probability of getting symptoms given that a person is vaccinated and has tested negative.\n",
    "$$p_1 = P(symptom | vaccine,covid') = \\frac{P(symptom \\cap vaccine,covid')}{P(vaccine,covid')} $$\n",
    "$$p_2 = P(symptom | vaccine',covid') = \\frac{P(symptom \\cap vaccine',covid')}{P(vaccine',covid')} $$\n",
    "\n",
    "We want to test the following hypothesis:\n",
    "$$ h_0: p_1 - p_2 \\leq 0 $$\n",
    "$$h_a: p_1 - p_2 > 0 $$\n",
    "\n",
    "We make a confidence intervall for each symptom, and reject the null hypothesis if confidence intervall does not include zero.\n",
    "\n",
    "**A large-sample 95% confidence interval for $p_1 - p_2$**:\n",
    "\n",
    "In this task it seems appropiate to estimate confidence intervalls instead of doing the task of comparison through hypotesis testing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The variables X and Y  represents the number of individuals in each sample having a certain  characteristic that defines $p_1$ and $p2$.Provided the population sizes are much larger than the sample sizes, the distribution\n",
    "of X can be choosen to be binomial with parameters m and $p_1$, and similarly, Y is choosen\n",
    "to be a binomial variable with parameters n and $p_2$. The samples are assumed to be independent of each other.Therefore X and Y are independent rv’s.\n",
    "\n",
    "The estimator for $ p_1-p_2 $, the difference in population proportions, is\n",
    "the  difference in sample proportions $\\frac{X}{m} - \\frac{Y}{n}$ .  $ \\hat{p_1} = \\frac{X}{m}$ and\n",
    "$\\hat{p_2} = \\frac{Y}{m}$, the estimator of $ p_1-p_2 $ can be expressed as $\\hat{p_1} - \\hat{p_2}$.\n",
    "\n",
    "$E[\\hat{p_1} - \\hat{p_2}]=p_1-p_2$ so $\\hat{p_1} - \\hat{p_2}$ is an unbiased estimate of $p_1-p_2$ \n",
    "\n",
    "$\\hat{p_1}=\\frac{X}{m}$ and $\\hat{p_2}=\\frac{y}{n}$ are aproximately normal distributed when m and n are large.\n",
    "\n",
    "\n",
    "Refrence: Modern Mathematical Statistic with applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388abee",
   "metadata": {},
   "source": [
    "## The effect of treatments on alleviating symptoms\n",
    "\n",
    "We are not interested in the outcomes for individuals who did not have any symptoms neither before nor after the treatment,since these individuals dont tell us anything about wether one had a positve of negative effect of the treatment.\n",
    "\n",
    "We do this modelling task in a similar way that we did the modelling for vaccine efficacy. Since we now have data from before and after a treatment, we calculate the ratio of people who still have symptoms after treatment to the people who had symptoms before treatment for both the treated group and the control group. \n",
    "\n",
    "\n",
    "\n",
    "We estimate the efficacy of the treatments based on this ration\n",
    "\n",
    "\n",
    "$t_1 = P(symptom = 1 | treatment = 1)$ after treatment, $t_0 = P(symptom = 1 | treatment = 1) $ before treatment.\n",
    "\n",
    "$n_1 = P(symptom = 1 | treatment = 0)$ after treatment, $n_0 = P(symptom = 1 | treatment = 0) $ before treatment\n",
    "\n",
    "$r_t = t_1 / t_0$ and\n",
    "\n",
    "$r_n = n_1 / n_0$\n",
    "\n",
    "\n",
    "$$ ratio=\\frac{r_n}{r_t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ef004",
   "metadata": {},
   "source": [
    "## The modelling\n",
    "\n",
    "We start by importing the libraries and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3898c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd83b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = pd.read_csv(\"observation_features.csv\")\n",
    "treat_data = pd.read_csv(\"treatment_features.csv\")\n",
    "action_data = pd.read_csv(\"treatment_actions.csv\")\n",
    "outcome_data = pd.read_csv(\"treatment_outcomes.csv\")\n",
    "\n",
    "cols = (['Covid-Recovered', 'Covid-Positive', 'No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death',\n",
    "        'Age', 'Gender', 'Income'] +\n",
    "         [f'Gene_{i+1:03}' for i in range(128)] +\n",
    "         ['Asthma', 'Obesity', 'Smoking', 'Diabetes', 'Heart disease', 'Hypertension',\n",
    "         'Vacc_1', 'Vacc_2', 'Vacc_3'])\n",
    "\n",
    "obs_data.columns = cols\n",
    "treat_data.columns = cols\n",
    "outcome_data.columns = cols[:10]\n",
    "action_data.columns = ['Treatment_1', 'Treatment_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e5b2d",
   "metadata": {},
   "source": [
    "## The pipelines\n",
    "\n",
    "In this section we indroduce **Pipeline_observational**. This pipeline will be used throughout problem. It contains the methods for modelling the effective genes, finding efficacy of the vaccines and the estimating the probability of sideeffects for all vaccines. We also introduce the **Pipeline_treatment** which contains methods for estimating the efficacy for treatments.\n",
    "\n",
    "\n",
    "Skrivvv mer detaljert......\n",
    "\n",
    "Put inn klassediagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c9f11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline_observational():\n",
    "    \n",
    "    def run_select_features(self,genes, symptom, threshold):\n",
    "        \"\"\"This function finds the selected features, then runs BIC test in order \n",
    "        to see whether the model with selected features are better than the full model\"\"\"\n",
    "        \n",
    "        self.best_features = best_features = self.select_features(genes,symptom,threshold)\n",
    "        self.important_genes = [col for col in genes.iloc[:,best_features].columns]\n",
    "\n",
    "    def select_features(self, X, Y, threshold):\n",
    "        \"\"\" Select the most important features of a data set, where X (2D)\n",
    "        contains the feature data, and Y (1D) contains the target\n",
    "        \"\"\"\n",
    "        X, Y = np.array(X), np.array(Y)\n",
    "        n_features = X.shape[1]\n",
    "        n_data =  X.shape[0]\n",
    "        alpha_b = np.ones([n_features, 2 ])\n",
    "        beta_b = np.ones([n_features, 2])\n",
    "        log_p = np.zeros(n_features)\n",
    "\n",
    "        log_null = 0\n",
    "        alpha = 1\n",
    "        beta = 1\n",
    "        for t in range(n_data):\n",
    "            p_null = alpha / (alpha + beta)\n",
    "            log_null += np.log(p_null)*Y[t] + np.log(1-p_null)*(1 - Y[t])\n",
    "            alpha += Y[t]\n",
    "            beta += (1 - Y[t])\n",
    "            for i in range(n_features):\n",
    "                x_ti = int(X[t,i])\n",
    "                p = alpha_b[i, x_ti] / (alpha_b[i, x_ti] + beta_b[i, x_ti])\n",
    "                log_p[i] += np.log(p)*Y[t] + np.log(1-p)*(1 - Y[t])\n",
    "                alpha_b[i, x_ti] += Y[t]\n",
    "                beta_b[i, x_ti] += (1 - Y[t])\n",
    "        log_max=np.mean(log_p)\n",
    "        log_max2=np.mean(log_null)\n",
    "        log_p=log_p-log_max\n",
    "        log_null=log_null-log_max2\n",
    "        p = 1 / (np.exp(log_null - log_p) + 1)\n",
    "        return [i for i in range(n_features) if p[i] > threshold]\n",
    "    \n",
    "    def estimate_gene_effect(self,symptom, genome):\n",
    "        \"\"\"Given a genome and a symptom, we estimate the probability of having a symptom given data\"\"\"\n",
    "        symptom, genome = np.array(symtom), np.array(genome)\n",
    "        alpha = 1\n",
    "        beta = 1\n",
    "        for t in range(genome.shape[0]):\n",
    "            if gene[t] == 1:\n",
    "                alpha += symptom[t]\n",
    "                beta += (1 - symptom[t])\n",
    "\n",
    "        return alpha/(alpha+beta)\n",
    "\n",
    "    def find_alpha(self, beta,p):\n",
    "        \"\"\" Given beta and a mean probability p, compute and return the alpha of a beta distribution. \"\"\"\n",
    "        return beta*p/(1-p)\n",
    "    \n",
    "    def find_efficacy(self, group_pos: pd.DataFrame, group_neg: pd.DataFrame, symptom, prior_probs):\n",
    "        \"\"\" Given a dataframe group_pos (vaccinated and covid positive) and group_neg (not vaccinated and covid postive) we estimate the efficacy of the vaccines\"\"\"\n",
    "        \n",
    "        if isinstance(symptom, int):\n",
    "            symptom_index = symptom\n",
    "            symptom_name = group_pos.keys()[symptom]\n",
    "        else:\n",
    "            symptom_name = symptom\n",
    "            symptom_index = list(group_pos.keys()).index(symptom)\n",
    "\n",
    "        group_pos_count = np.sum(group_pos[symptom_name] * group_pos.iloc[:,1])\n",
    "        group_neg_count = np.sum(group_neg[symptom_name] * group_neg.iloc[:,1])\n",
    "\n",
    "        v = group_pos_count/len(group_pos)\n",
    "        n_v = group_neg_count/len(group_neg)\n",
    "\n",
    "        if n_v == 0:\n",
    "            print(f'{v=}, {n_v=}: Division by zero')\n",
    "            return\n",
    "\n",
    "        IRR = v/n_v\n",
    "\n",
    "        #print(v, n_v)\n",
    "        efficacy = 100*(1- IRR)\n",
    "\n",
    "        N = 100_000\n",
    "        beta = 1\n",
    "        p = prior_probs[symptom_index]\n",
    "        alpha = self.find_alpha(beta,p)\n",
    "\n",
    "        samples_group_pos = stats.beta.rvs(alpha + group_pos_count, beta + len(group_pos) - group_pos_count, size=N)\n",
    "        samples_group_neg = stats.beta.rvs(alpha + group_neg_count, beta + len(group_neg) - group_neg_count, size=N)\n",
    "\n",
    "        samples_ve = 100 * (1 - samples_group_pos/samples_group_neg)\n",
    "        lower = np.percentile(samples_ve, 2.5)\n",
    "        upper = np.percentile(samples_ve, 97.5)\n",
    "        \n",
    "        if efficacy >= lower and efficacy <= upper:\n",
    "            status = 'not rejected'\n",
    "        else:\n",
    "            status = 'rejected'\n",
    "            \n",
    "        print(f'{symptom_name:15s}: {efficacy:3.3f} - ({lower:3.3f}, {upper:3.3f}) - {status}')\n",
    "    \n",
    "    def run_efficacy(self, vacced, un_vacced,prior_probs):\n",
    "        \"\"\"Given a set of vaccinated and unvaccinated data, we estimate the efficacy of all vaccines together and then one by one \"\"\"\n",
    "        for i, s in enumerate(self.symptom_names):\n",
    "            self.find_efficacy(vacced,un_vacced,i,prior_probs)\n",
    "        print(\"\")\n",
    "        \n",
    "        vacc_type1 = self.obs_data[obs_data.Vacc_1 == 1]\n",
    "        vacc_type2 = self.obs_data[obs_data.Vacc_2 == 1]\n",
    "        vacc_type3 = self.obs_data[obs_data.Vacc_3 == 1]\n",
    "        vaccination_types = [vacc_type1,vacc_type2,vacc_type3]\n",
    "        vaccination_names = ['type 1', 'type 2', 'type 3']\n",
    "        \n",
    "        for name in vaccination_names:\n",
    "            print(name)\n",
    "            index = vaccination_names.index(name)\n",
    "            for i, s in enumerate(self.symptom_names):\n",
    "                self.find_efficacy(vaccination_types[index],un_vacced,i,prior_probs)\n",
    "            print(\"\")\n",
    "        \n",
    "        \n",
    "    def side_effects(self, vacced_neg, un_vacced_neg, start, end):\n",
    "        \"\"\"Given a set of vaccinated and unvaccinated data, we estimate the efficacy of the vaccines\"\"\"\n",
    "\n",
    "        df = pd.DataFrame(index=vacced_neg.keys()[start:end],\n",
    "                          columns = (\"p1 (%)\", \"p2 (%)\", \"Diff (%)\", \"Credible Interval (%)\", \"Null Hypothesis\", ),\n",
    "                         )\n",
    "\n",
    "        for i in range(start, end):\n",
    "            symptom = vacced_neg.keys()[i]\n",
    "            p1 = vacced_neg.sum()[i] / len(self.y) / (len(vacced_neg) / len(self.y))\n",
    "            p2 = un_vacced_neg.sum()[i] / len(self.y) / (len(un_vacced_neg) / len(self.y))\n",
    "\n",
    "\n",
    "            lower = (p1-p2 - 1.64 * np.sqrt((p1*(1-p1) / len(vacced_neg)) + (p2 * (1-p2) / len(un_vacced_neg))))\n",
    "            higher = (p1-p2 + 1.64 * np.sqrt((p1*(1-p1) / len(vacced_neg)) + (p2 * (1-p2) / len(un_vacced_neg))))\n",
    "\n",
    "            p1, p2, lower, higher = p1 * 100, p2 * 100, lower * 100, higher * 100\n",
    "\n",
    "            df.loc[symptom] = np.array([round(p1, 4), round(p2, 4), round(p1 - p2, 4), (round(lower, 4), round(higher, 4)),\n",
    "                               \"rejected\" if lower>0 else \"not rejected\", ],dtype=object)\n",
    "\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e61c7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline_treatment():\n",
    "    \n",
    "    def __init__(self,treat_data,action_data,outcome_data):\n",
    "        \"\"\"Given the pre treatment symptoms, the treatment action and post treatment symptoms, we slice the data into actionstypes and outcome types \"\"\"\n",
    "\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        self.treat_data = treat_data\n",
    "        self.action_data = action_data\n",
    "        self.outcome_data = outcome_data\n",
    "\n",
    "        new_treat_data = treat_data[((np.sum(treat_data.iloc[:,2:10],axis=1) > 0.0) | np.sum(outcome_data.iloc[:,2:10],axis=1) > 0.0)]\n",
    "        self.group_first = new_treat_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 0))]\n",
    "        self.group_second = new_treat_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 1))]\n",
    "        self.group_both = new_treat_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 1))]\n",
    "        self.group_none = new_treat_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 0))]\n",
    "\n",
    "        new_outcome_data = outcome_data[((np.sum(treat_data.iloc[:,2:10],axis=1) > 0.0) | np.sum(outcome_data.iloc[:,2:10],axis=1) > 0.0)]\n",
    "        self.outcome_first = new_outcome_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 0))]\n",
    "        self.outcome_second = new_outcome_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 1))]\n",
    "        self.outcome_both = new_outcome_data[((action_data.iloc[:,0] == 1) & (action_data.iloc[:,1] == 1))]\n",
    "        self.outcome_none = new_outcome_data[((action_data.iloc[:,0] == 0) & (action_data.iloc[:,1] == 0))]\n",
    "        self.prior_probs= [(np.sum(new_treat_data[sym]) + np.sum(new_outcome_data[sym])) / (len(new_treat_data) * 2) for sym in outcome_data.columns][2:]\n",
    "\n",
    "    def print_treatment_efficacy(self):\n",
    "        outcome_first, outcome_second, outcome_both, outcome_none, group_first, group_second, group_both, group_none = self.outcome_first, self.outcome_second, self.outcome_both, self.outcome_none, self.group_first, self.group_second, self.group_both, self.group_none\n",
    "        for outcome_treated, pre_treated, treatment in zip([outcome_first, outcome_second, outcome_both],[group_first, group_second, group_both],['treatment 1', 'treatment 2', 'both treatments']):\n",
    "            print(f\"{treatment} efficacy:\")\n",
    "            for i, key in enumerate(self.outcome_data.keys()[2:]):\n",
    "                #print(key)\n",
    "                self.treatment_efficacy(outcome_treated, pre_treated, outcome_none, group_none, self.prior_probs[i], key)\n",
    "            print()\n",
    "\n",
    "\n",
    "    def find_alpha(self, beta,p):\n",
    "        \"\"\" Given beta and a mean probability p, compute and return the alpha of a beta distribution. \"\"\"\n",
    "        return beta*p/(1-p)\n",
    "\n",
    "    def treatment_efficacy(self, outcome_treated, precondition_treated, outcome_untreated, precondition_untreated, p, symptom_name, log=True):\n",
    "        \n",
    "        group_pos_count = np.sum(outcome_treated[symptom_name])\n",
    "        group_neg_count = np.sum(outcome_untreated[symptom_name])\n",
    "\n",
    "        group_pos_total = np.sum(precondition_treated[symptom_name])\n",
    "        group_neg_total = np.sum(precondition_untreated[symptom_name])\n",
    "\n",
    "        if any(v == 0 for v in (group_pos_total, group_neg_total, group_neg_count)):\n",
    "            print(f'{symptom_name:15s}: Division by zero - not enough data to compute efficacy' )\n",
    "            return\n",
    "\n",
    "        v = group_pos_count / group_pos_total\n",
    "        n_v = group_neg_count / group_neg_total\n",
    "        IRR = v/n_v\n",
    "\n",
    "        efficacy = 100 * (1- IRR)\n",
    "\n",
    "        N = 100_000\n",
    "        beta = 1\n",
    "        alpha = self.find_alpha(beta,p)\n",
    "\n",
    "        #symptom_name = symptom_names[symptom_index]\n",
    "        samples_group_pos = stats.beta.rvs(alpha + group_pos_count, beta + len(outcome_treated) - group_pos_count, size=N)\n",
    "        samples_group_neg = stats.beta.rvs(alpha + group_neg_count, beta + len(outcome_untreated) - group_neg_count, size=N)\n",
    "\n",
    "        samples_ve = 100 * (1 - samples_group_pos/samples_group_neg)\n",
    "        lower = np.percentile(samples_ve, 2.5)\n",
    "        upper = np.percentile(samples_ve, 97.5)\n",
    "        if log is True:\n",
    "            print(f'{symptom_name:15s}: {efficacy:7.3f} - 95% CI: ({lower:3.3f}, {upper:3.3f})')\n",
    "\n",
    "        return efficacy, (lower, upper)\n",
    "    \n",
    "    \n",
    "    def plot_symptoms(self):\n",
    "        pre_treatment = {}\n",
    "        treatment_one = {}\n",
    "        treatment_two = {}\n",
    "        treatment_both = {}\n",
    "        treatment_none = {}\n",
    "\n",
    "        for symptom_name in self.treat_data.columns[2:10]:\n",
    "            pre_treatment[symptom_name] = np.sum(self.treat_data[symptom_name])\n",
    "            treatment_one[symptom_name] = np.sum(self.outcome_first[symptom_name])\n",
    "            treatment_two[symptom_name] = np.sum(self.outcome_second[symptom_name])\n",
    "            treatment_both[symptom_name] = np.sum(self.outcome_both[symptom_name])\n",
    "            treatment_none[symptom_name] = np.sum(self.outcome_none[symptom_name])\n",
    "\n",
    "        x_axis = np.arange(len(pre_treatment.keys()))\n",
    "        plt.bar(x_axis-0.24, pre_treatment.values(), width = 0.12, color='b', label = \"Pre treatment\")\n",
    "        plt.bar(x_axis-0.12, treatment_one.values(), width = 0.12, color='g', label = \"Treatment 1\")\n",
    "        plt.bar(x_axis, treatment_two.values(), width = 0.12,color='r', label = \"Treatment 2\")\n",
    "        plt.bar(x_axis+0.12, treatment_both.values(), width = 0.12,color='purple', label = \"Both treatments\")\n",
    "        plt.bar(x_axis+0.24, treatment_none.values(), width = 0.12,color='pink', label = \"No treatment\")\n",
    "        plt.xticks(x_axis, self.treat_data.columns[2:10], rotation=90)\n",
    "        plt.ylabel(\"Number of occurances\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46062f6",
   "metadata": {},
   "source": [
    "Most of the the individuals did'nt have any symptoms. We can theirfore see that the classifier we used failed to classify all the individuals that did have symptoms.The only way one can then determain if our select feuture method works or not is using generated data,that are highly squed toward individuals having symptoms.\n",
    "\n",
    "Our result also shows that genes in them selves are not very good determiners for wether somebody that are covid positive gets a certain symptom or not. These types of conclutions can only be made using the original data. \n",
    "\n",
    "To see if their is somthing \"wrong\" with the algorithm one could try different algorithms.If the result still is the same, \n",
    "genes in them selves are not very good determiners for wether somebody that are covid positive gets a certain symptom or not.\n",
    "\n",
    "\n",
    "Only use generated data to confirm wether the select feuture method works as it is expected to work.You cant alter reality as you want,generating data to force feutures to be significant,does not reflect reality,it reflects what you want reality to be.\n",
    "\n",
    "Could test gender and comorbidities which also is binary variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a54d89",
   "metadata": {},
   "source": [
    "## Generating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcfaf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_data(num_features, N, correlation=[0.9, 0.5]):\n",
    "    data = np.random.choice(2, size=(N, num_features))\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"Target\"] = np.zeros(N).astype(int)\n",
    "    for i, cor in enumerate(correlation):\n",
    "        if i >= num_features:\n",
    "            break\n",
    "\n",
    "        df[\"Target\"] |= df.iloc[:, i] * np.random.choice(2, size=N, p=[(1-cor), cor])\n",
    "\n",
    "    return df.iloc[:, :num_features], df[\"Target\"]\n",
    "\n",
    "def generate_genomes_symptoms(random_indecies):\n",
    "    cor = [0.2 for _ in range(128)]\n",
    "    for r in random_indecies:\n",
    "        cor[r] = 0.9\n",
    "    X,y = generate_binary_data(128,100_000, correlation=cor)\n",
    "    X.columns = [f'Gene_{i+1:03}' for i in range(128)]\n",
    "    y.columns = cols[1]\n",
    "    return X,y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8189fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vaccine_data():\n",
    "    sym_g = np.random.choice(2,size=[100_000,10])\n",
    "    y = np.random.choice(2,size=[100_000,1])\n",
    "\n",
    "    vac_g = np.zeros([100_000,3])\n",
    "    for i,v in enumerate(y):\n",
    "        if v == 1:\n",
    "            rand_ind = random.randint(0,2)\n",
    "            vac_g[i][rand_ind] = 1\n",
    "\n",
    "    vac_g = pd.DataFrame(vac_g,columns = ['Vacc_1', 'Vacc_2', 'Vacc_3'])\n",
    "    sym_g = pd.DataFrame(sym_g,columns = ['Covid-Recovered', 'Covid-Positive', 'No-Taste/Smell', 'Fever', 'Headache', 'Pneumonia', 'Stomach', 'Myocarditis', 'Blood-Clots', 'Death'])\n",
    "\n",
    "    vacced = sym_g[np.sum(vac_g.iloc[:,-3:], axis=1) == 1]\n",
    "    un_vacced = sym_g[np.sum(vac_g.iloc[:,-3:], axis=1) == 0]\n",
    "    prior_probs_generated = [np.sum(sym_g.iloc[:,i]) / len(sym_g) for i, key in enumerate(sym_g.columns)]\n",
    "    return vacced, un_vacced, prior_probs_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2075b5e",
   "metadata": {},
   "source": [
    "### The experiment setup\n",
    "\n",
    "In this section we will set up the experiment by running the pipeline with different generated data and see whether it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9de483ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run nr: 0\n",
      "Run nr: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xc/9tjchlhd1yn758nk26c1m1dc0000gn/T/ipykernel_23319/2331037690.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgenomes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymptom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_genomes_symptoms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_indecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline_observational\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_select_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenomes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymptom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xc/9tjchlhd1yn758nk26c1m1dc0000gn/T/ipykernel_23319/4257302061.py\u001b[0m in \u001b[0;36mrun_select_features\u001b[0;34m(self, genes, symptom, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m         to see whether the model with selected features are better than the full model\"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msymptom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportant_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/xc/9tjchlhd1yn758nk26c1m1dc0000gn/T/ipykernel_23319/4257302061.py\u001b[0m in \u001b[0;36mselect_features\u001b[0;34m(self, X, Y, threshold)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mx_ti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ti\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ti\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ti\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mlog_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0malpha_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_ti\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Run nr: {i}')\n",
    "    random_indecies = random.sample(range(128), 20)\n",
    "    genomes,symptom = generate_genomes_symptoms(random_indecies)\n",
    "    pipe = Pipeline_observational()\n",
    "    pipe.run_select_features(genomes,symptom,0.8)\n",
    "    \n",
    "    for i in pipe.best_features:\n",
    "        if i not in random_indecies:\n",
    "            print(f'{i}: fail')\n",
    "\n",
    "\n",
    "    #kjøre efficacy\n",
    "    #kjøre sideeffects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ab10a",
   "metadata": {},
   "source": [
    "## Running pipeline on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cb868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54d69ca6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
